# AI Coding Agent Instructions for Monte_Carlo Project

## Project Overview
The Monte_Carlo project simulates particle interactions using the Monte Carlo method. The codebase includes multiple Python scripts, each representing different steps or weeks of development. The project appears to focus on simulating particle trajectories, including mechanisms like reflection.

### Key Features:
- **Particle Reflection Mechanism**: A critical feature, recently debugged, ensuring accurate simulation of particle behavior.
- **Step-wise Development**: Scripts are organized by steps (e.g., `First_step1.py`, `Second_step.py`) and weeks (e.g., `Week2_Particle_angle.py`, `week5.py`), indicating iterative development.
- **Visualization**: The `Picture/` directory contains output images, suggesting the project generates visualizations of particle trajectories.

## Codebase Structure
- **Main Scripts**: Each script (e.g., `First_step1.py`, `week5.py`) likely represents a distinct phase of the simulation.
- **Visualization Outputs**: The `Picture/` directory stores images generated by the simulations.
- **Testing**: `test.py` may contain test cases or debugging utilities.
- **Documentation**: Minimal documentation is available in `README.md`.

## Development Conventions
- **File Naming**: Scripts are named sequentially or by week, reflecting their role in the simulation pipeline.
- **Bug Fixes**: Recent updates focused on fixing the particle reflection mechanism.

## Recommendations for AI Agents
1. **Understand the Simulation Pipeline**:
   - Review scripts in sequential order (e.g., `First_step1.py`, `Second_step.py`) to understand the data flow and simulation logic.
   - Analyze `week5.py` for the latest developments.

2. **Focus on Particle Reflection**:
   - Pay attention to code handling particle reflection, as it is a critical and recently debugged feature.

3. **Leverage Visualization Outputs**:
   - Use images in `Picture/` to validate simulation results.

4. **Testing and Debugging**:
   - Investigate `test.py` for existing test cases or debugging utilities.

5. **Extend Documentation**:
   - Add comments and docstrings to clarify simulation logic and parameters.

## Key Files and Directories
- **Scripts**: `First_step1.py`, `Second_step.py`, `week5.py`
- **Visualization Outputs**: `Picture/`
- **Testing**: `test.py`
- **Documentation**: `README.md`

## Example Tasks for AI Agents
- Debug issues in the particle reflection mechanism.
- Add comments to clarify the purpose of each script.
- Generate new visualizations and save them in `Picture/`.
- Write test cases for untested parts of the simulation.

## Notes
- The project structure and naming conventions suggest iterative development. Ensure changes align with the existing workflow.
- Validate simulation results against the visualizations in `Picture/`.

---
This guide is a starting point. Update it as the project evolves.